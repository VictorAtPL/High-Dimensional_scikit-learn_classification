{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import category_encoders as ce\n",
    "import collections\n",
    "%matplotlib inline\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xgboost\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"3rd_party/train.txt\"\n",
    "test_filename = \"3rd_party/testx.txt\"\n",
    "\n",
    "df_train = pd.read_csv(train_filename, sep=\" \")\n",
    "df_testx = pd.read_csv(test_filename, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model\n",
    "Y_train = df_train['class']\n",
    "X_train = df_train.drop(['class'], axis=1)\n",
    "\n",
    "Y_test = None\n",
    "X_test = df_testx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_with_na(df_train, df_test, threshold = 1.00):\n",
    "    na_fraq = df_train.apply(lambda x: sum(x.isnull()) / len(df_train))\n",
    "    all_na_indices = na_fraq[na_fraq >= threshold].index\n",
    "    df_train = df_train.drop(all_na_indices, axis=1)\n",
    "    df_test = df_test.drop(all_na_indices, axis=1)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def drop_with_no_variance(df_train, df_test):\n",
    "    variance = df_train.var()\n",
    "    all_no_variance = variance[variance == 0.0].index\n",
    "    df_train = df_train.drop(all_no_variance, axis=1)\n",
    "    df_test = df_test.drop(all_no_variance, axis=1)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def drop_with_low_correlation(df_train_X, df_test, threshold = 0.10, df_train_Y=None):\n",
    "    assert df_train_Y is not None\n",
    "    \n",
    "    df_train = pd.concat([df_train_X, df_train_Y], axis=1)\n",
    "    # Compute the correlation matrix\n",
    "    corr = df_train.corr()\n",
    "\n",
    "    all_high_correlation = corr['class'][corr['class'].abs() >= threshold].drop('class').index\n",
    "    df_train_X = df_train_X[all_high_correlation]\n",
    "    df_test = df_test[all_high_correlation]\n",
    "    \n",
    "    return df_train_X, df_test\n",
    "    \n",
    "def drop_categorical(df_train, df_test, threshold = 30):\n",
    "    var_types = df_train.dtypes\n",
    "    all_categorical_vars = var_types[var_types == \"object\"].index\n",
    "    df_only_categorical_vars = df_train[all_categorical_vars]\n",
    "    df_categorical_vars_count = df_only_categorical_vars.nunique().sort_values(ascending=False)\n",
    "    \n",
    "    categorical_vars_to_drop = df_categorical_vars_count[df_categorical_vars_count > threshold].index\n",
    "    \n",
    "    df_train = df_train.drop(categorical_vars_to_drop, axis=1)\n",
    "    df_test = df_test.drop(categorical_vars_to_drop, axis=1)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def encode_categorical(df_train, df_test, df_train_Y=None):\n",
    "    assert df_train_Y is not None\n",
    "    \n",
    "    var_types = df_train.dtypes\n",
    "    all_categorical_vars = var_types[var_types == \"object\"].index\n",
    "    encoder_target = ce.TargetEncoder(cols=all_categorical_vars.values)\n",
    "    mapping = {before: before + \"_te\" for before in all_categorical_vars.values}\n",
    "    df_train = encoder_target.fit_transform(df_train, df_train_Y).rename(columns=mapping)\n",
    "    df_test = encoder_target.transform(df_test).rename(columns=mapping)\n",
    "    \n",
    "    return df_train, df_test    \n",
    "\n",
    "def add_na_indicator(df_train, df_test, threshold=0.25):\n",
    "    na_fraq = df_train.apply(lambda x: sum(x.isnull()) / len(df_train))\n",
    "    vars_with_na_above_threshold = na_fraq[na_fraq >= threshold].index\n",
    "\n",
    "    missing_indicator = MissingIndicator()\n",
    "\n",
    "    ## train\n",
    "    missing_indicator_val = missing_indicator.fit_transform(df_train[vars_with_na_above_threshold]).astype(int)\n",
    "    df_missing_indicators = pd.DataFrame(missing_indicator_val, columns=[col + \"_was_na\" for col in vars_with_na_above_threshold]).set_index(df_train.index)\n",
    "\n",
    "    df_train = pd.concat([df_train, df_missing_indicators], axis=1).drop(vars_with_na_above_threshold, axis=1)\n",
    "    \n",
    "    ## test\n",
    "    missing_indicator_val = missing_indicator.transform(df_test[vars_with_na_above_threshold]).astype(int)\n",
    "    df_missing_indicators = pd.DataFrame(missing_indicator_val, columns=[col + \"_was_na\" for col in vars_with_na_above_threshold]).set_index(df_test.index)\n",
    "\n",
    "    df_test = pd.concat([df_test, df_missing_indicators], axis=1).drop(vars_with_na_above_threshold, axis=1)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def remove_correlated_features(df_train, df_test, threshold=0.2):\n",
    "    while True:\n",
    "        corr_matrix = df_train.corr().abs()\n",
    "\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "        if len(to_drop) == 0:\n",
    "            break\n",
    "            \n",
    "        df_train = df_train.drop(to_drop[0], axis=1)\n",
    "        df_test = df_test.drop(to_drop[0], axis=1)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def replace_na_by_mean(df_train, df_test):\n",
    "    df_test = df_test.fillna(df_train.mean(axis=0))\n",
    "    df_train = df_train.fillna(df_train.mean(axis=0))\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def normalize_data(df_train, df_test):\n",
    "    columns_to_not_normalize = [column for column in df_train.columns.values if any(x in column for x in (\"_te\", \"_was_na\"))]\n",
    "    columns_mean = df_train.mean(axis=0)\n",
    "    columns_mean[columns_to_not_normalize] = 0\n",
    "    columns_std = df_train.std(axis=0)\n",
    "    columns_std[columns_to_not_normalize] = 1\n",
    "    \n",
    "    df_test = (df_test - columns_mean) / columns_std\n",
    "    df_train = (df_train - columns_mean) / columns_std\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def prepare_dfs(df_train_X, df_test, df_train_Y):\n",
    "    df_train_X, df_test = drop_with_na(df_train_X, df_test, threshold=1.0)\n",
    "    df_train_X, df_test = drop_with_no_variance(df_train_X, df_test)\n",
    "    df_train_X, df_test = drop_categorical(df_train_X, df_test, threshold=30)\n",
    "    df_train_X, df_test = encode_categorical(df_train_X, df_test, df_train_Y)\n",
    "    df_train_X, df_test = add_na_indicator(df_train_X, df_test, threshold=0.6)\n",
    "    df_train_X, df_test = replace_na_by_mean(df_train_X, df_test)\n",
    "    df_train_X, df_test = normalize_data(df_train_X, df_test)\n",
    "    df_train_X, df_test = drop_with_low_correlation(df_train_X, df_test, df_train_Y=df_train_Y, threshold=0.03)\n",
    "    df_train_X, df_test = remove_correlated_features(df_train_X, df_test, threshold=0.25)\n",
    "    \n",
    "    return df_train_X, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(row):\n",
    "    if row['0'] >= row['1']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_accuracy_top_10(predictions, real_class):\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['0', '1'])\n",
    "    predicted_class = predictions_df.apply(lambda row: predict_class(row), axis=1)\n",
    "    predictions_df['predicted_class'] = predicted_class\n",
    "    predictions_df['real_class'] = real_class.reset_index(drop=True)\n",
    "    top_10_percent_predictions_count = predictions_df.shape[0] // 10\n",
    "    top_10_percent_predictions_df = predictions_df.nlargest(top_10_percent_predictions_count, '1')\n",
    "    same_class = top_10_percent_predictions_df[top_10_percent_predictions_df['real_class'] == top_10_percent_predictions_df['predicted_class']]\n",
    "    one_class = same_class[same_class['real_class'] == 1]\n",
    "    return float(one_class.count().values[0]) / float(top_10_percent_predictions_count)\n",
    "\n",
    "def get_accuracy(predictions, real_class):\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['0', '1'])\n",
    "    predicted_class = predictions_df.apply(lambda row: predict_class(row), axis=1)\n",
    "    \n",
    "    return accuracy_score(real_class, predicted_class.values)\n",
    "\n",
    "def get_kfold(X, Y, n_splits=5, shuffle=True, random_state=200):\n",
    "    dfs = []\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    for train_indices, val_indices in kf.split(X, Y):\n",
    "        X_train_local = X.take(train_indices)\n",
    "        Y_train_local = Y.take(train_indices)\n",
    "        X_val_local = X.take(val_indices)\n",
    "        Y_val_local = Y.take(val_indices)\n",
    "\n",
    "        dfs.append((X_train_local, Y_train_local, X_val_local, Y_val_local))\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "def prepare_cv(dfs):\n",
    "    prepared_dfs = []\n",
    "    \n",
    "    for X_train_local, Y_train_local, X_val_local, Y_val_local in tqdm(dfs):\n",
    "        X_train_local, X_val_local = prepare_dfs(X_train_local, X_val_local, Y_train_local)\n",
    "        prepared_dfs.append((X_train_local, Y_train_local, X_val_local, Y_val_local))\n",
    "    \n",
    "    return prepared_dfs\n",
    "\n",
    "def get_model_cv_score(prepared_dfs, model, rounds=None):\n",
    "    top_10_accuracies_train = []\n",
    "    top_10_accuracies_val = []\n",
    "    output = []\n",
    "\n",
    "    iterator = 1\n",
    "    for X_train_local, Y_train_local, X_val_local, Y_val_local in tqdm(prepared_dfs):\n",
    "        fullargspec = inspect.getfullargspec(model.fit)\n",
    "        if 'sample_weight' in fullargspec.args:\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=Y_train_local)\n",
    "            model = model.fit(X_train_local, Y_train_local, sample_weight=sample_weight)\n",
    "        else:\n",
    "            model = model.fit(X_train_local, Y_train_local)\n",
    "            \n",
    "        output.append(model)\n",
    "        predictions_probs_train = model.predict_proba(X_train_local)\n",
    "        predictions_probs_val = model.predict_proba(X_val_local)\n",
    "        \n",
    "        top_10_accuracy_train = get_accuracy_top_10(predictions_probs_train, Y_train_local)\n",
    "        top_10_accuracy_val = get_accuracy_top_10(predictions_probs_val, Y_val_local)\n",
    "        #top_10_accuracy_train = get_accuracy(predictions_probs_train, Y_train_local)\n",
    "        #top_10_accuracy_val = get_accuracy(predictions_probs_val, Y_val_local)\n",
    "        top_10_accuracies_train.append(top_10_accuracy_train)\n",
    "        top_10_accuracies_val.append(top_10_accuracy_val)\n",
    "        \n",
    "        if rounds is not None and iterator == rounds:\n",
    "            break\n",
    "        iterator += 1\n",
    "    \n",
    "    return top_10_accuracies_train, top_10_accuracies_val, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CV\n",
    "dfs_cv = get_kfold(X_train, Y_train, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preprocessing\n",
    "prepared_dfs = prepare_cv(dfs_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not important columns, so that ones which doesnt occure in all of prepared_dfs\n",
    "indices_sum = [column for prepared_df in prepared_dfs for column in prepared_df[0].columns.values]\n",
    "counter = collections.Counter(indices_sum)\n",
    "counter_series = pd.Series(dict(counter))\n",
    "not_important_columns = set(counter_series[counter_series < counter_series.max()].index.values)\n",
    "\n",
    "for prepared_df in prepared_dfs:\n",
    "    pick_columns = set(prepared_df[0].columns) & not_important_columns\n",
    "    prepared_df[0].drop(list(pick_columns), axis=1, inplace=True)\n",
    "    prepared_df[2].drop(list(pick_columns), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(n_jobs=-1, solver='saga', C=0.01, max_iter=1e6, penalty='l2') # train: 0.262, val: 0.260\n",
    "#model = MLPClassifier(early_stopping=True, hidden_layer_sizes=(50), activation='relu', max_iter=200) # train: 0.294, val: 0.290\n",
    "#model = RandomForestClassifier(n_estimators=1000, n_jobs=-1, min_samples_split=200, max_depth=3) # train: 0.406, val: 0.401\n",
    "model = xgboost.XGBClassifier(objective=\"binary:logistic\", n_estimators=50, n_jobs=-1) # train: 0.400, val: 0.396\n",
    "\n",
    "top_10_accuracies_train, top_10_accuracies_val, output = get_model_cv_score(prepared_dfs, model, rounds=None)\n",
    "top_10_overall_accuracy_train = np.mean(top_10_accuracies_train)\n",
    "top_10_overall_accuracy_val = np.mean(top_10_accuracies_val)\n",
    "print(\"average of CV lift10 score, train: {0:.3f}, val: {1:.3f}\".format(top_10_overall_accuracy_train, top_10_overall_accuracy_val))\n",
    "print(top_10_accuracies_train, top_10_accuracies_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axs = plt.subplots(3, 4, figsize=(18, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    \n",
    "for i, prepared_df in enumerate(prepared_dfs):\n",
    "    # Compute the correlation matrix\n",
    "    corr = pd.concat([prepared_df[0], prepared_df[1]], axis=1).corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "                ax=axs[i // 4][i % 4], cbar_ax=axs[2][2])\n",
    "    ticks = np.arange(corr.shape[0]) + 0.5\n",
    "    axs[i // 4][i % 4].set_xticks(ticks)\n",
    "    axs[i // 4][i % 4].set_xticklabels(corr.columns, rotation=90, fontsize=8)\n",
    "    axs[i // 4][i % 4].set_yticks(ticks)\n",
    "    axs[i // 4][i % 4].set_yticklabels(corr.index, rotation=360, fontsize=8)\n",
    "\n",
    "    axs[i // 4][i % 4].set_title('correlation matrix kfold {}'.format(i + 1))\n",
    "\n",
    "axs[2][3].remove();\n",
    "\n",
    "plt.savefig('correlation_matrix_cv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_prediction(output, prepared_dfs):\n",
    "    predictions_probs_test = None\n",
    "    for (model, prepared_df) in zip(output, prepared_dfs):\n",
    "        _, _, X_test_local, _ = prepared_df\n",
    "        if predictions_probs_test is None:\n",
    "            predictions_probs_test = model.predict_proba(X_test_local)\n",
    "        else:\n",
    "            predictions_probs_test += model.predict_proba(X_test_local)\n",
    "            \n",
    "    predictions_probs_test /= float(len(output))\n",
    "    return predictions_probs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preprocessing\n",
    "dfs_cv_with_test = [(X_train_local, Y_train_local, X_test, Y_test) for (X_train_local, Y_train_local, _, _) in dfs_cv]\n",
    "prepared_dfs = prepare_cv(dfs_cv_with_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prepared_df in prepared_dfs:\n",
    "    pick_columns = set(prepared_df[0].columns) & not_important_columns\n",
    "    prepared_df[0].drop(list(pick_columns), axis=1, inplace=True)\n",
    "    prepared_df[2].drop(list(pick_columns), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_probs_test = get_models_prediction(output, prepared_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_probs_test[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name = \"PIOPOD\"\n",
    "filename_output = author_name + \".txt\"\n",
    "\n",
    "with open(filename_output, \"w\") as file: \n",
    "    file.write('\"{}\"\\n'.format(author_name))\n",
    "    \n",
    "    file.writelines([prob + \"\\n\" for prob in predictions_probs_test[:, 1].astype(str)])\n",
    "    #for prob in predictions_probs_test[:, 1]:\n",
    "    #    file.write(prob)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
